{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef2066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef6e36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native-country</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>132618</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>12</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>132655</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>?</td>\n",
       "      <td>132674</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>8</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>Private</td>\n",
       "      <td>132642</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>132646</td>\n",
       "      <td>Assoc-voc</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  age workclass  fnlwgt     education  education-num  \\\n",
       "0      0   22   Private  132618  Some-college             12   \n",
       "1      1   22   Private  132655     Bachelors              9   \n",
       "2      2   23         ?  132674       HS-grad              8   \n",
       "3      3   36   Private  132642     Bachelors             13   \n",
       "4      4   49   Private  132646     Assoc-voc             13   \n",
       "\n",
       "       marital-status       occupation   relationship   race     sex  \\\n",
       "0       Never-married     Tech-support      Own-child  White  Female   \n",
       "1  Married-civ-spouse  Farming-fishing        Husband  White    Male   \n",
       "2       Never-married    Other-service  Not-in-family  White  Female   \n",
       "3  Married-civ-spouse            Sales        Husband  White    Male   \n",
       "4  Married-civ-spouse  Exec-managerial        Husband  White    Male   \n",
       "\n",
       "  native-country  Y  \n",
       "0  United-States  0  \n",
       "1         Mexico  1  \n",
       "2  United-States  0  \n",
       "3  United-States  1  \n",
       "4  United-States  1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## column\n",
    "'workclass', 'education','marital-status', 'occupation', 'relationship', 'race', 'sex','native-country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8138783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train.drop(['Y'], axis=1)\n",
    "train_y=train['Y']\n",
    "test_x=test.copy()\n",
    "\n",
    "cat_cols= ['workclass', 'education','marital-status', 'occupation', 'relationship', 'race', 'sex','native-country']\n",
    "\n",
    "train_x = train_x.drop(['fnlwgt'],axis=1)\n",
    "train_x = train_x.drop(['race'],axis=1)\n",
    "\n",
    "cat_cols= ['workclass', 'education','marital-status', 'occupation', 'relationship', 'sex','native-country']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# labelencoder\n",
    "\n",
    "# ラベルエンコーディング（OrdinalEncoder）\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "cat_cols= ['workclass', 'education','marital-status', 'occupation', 'relationship', 'race', 'sex','native-country']\n",
    "\n",
    "train_x = train_x.dropna()\n",
    "test_x = test_x.dropna()\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# target encoding\n",
    "# -----------------------------------\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 変数をループしてtarget encoding\n",
    "for c in cat_cols:\n",
    "    # 学習データ全体で各カテゴリにおけるtargetの平均を計算\n",
    "    data_tmp = pd.DataFrame({c: train_x[c], 'target': train_y})\n",
    "    target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "    # テストデータのカテゴリを置換\n",
    "    test_x[c] = test_x[c].map(target_mean)\n",
    "\n",
    "    # 学習データの変換後の値を格納する配列を準備\n",
    "    tmp = np.repeat(np.nan, train_x.shape[0])\n",
    "\n",
    "    # 学習データを分割\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=72)\n",
    "    for idx_1, idx_2 in kf.split(train_x):\n",
    "        # out-of-foldで各カテゴリにおける目的変数の平均を計算\n",
    "        target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "        # 変換後の値を一時配列に格納\n",
    "        tmp[idx_2] = train_x[c].iloc[idx_2].map(target_mean)\n",
    "\n",
    "    # 変換後のデータで元の変数を置換\n",
    "    train_x[c] = tmp\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_profiling.profile_report"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pandas_profiling.ProfileReport(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop_col\n",
    "'fnlwgt',''race', 'df_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelencoder\n",
    "\n",
    "# ラベルエンコーディング（OrdinalEncoder）\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "\"\"\"\n",
    "'index', 'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "       'native-country',\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "train_x = train_x.dropna()\n",
    "test_x = test_x.dropna()\n",
    "\n",
    "train_x.head()\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# target encoding\n",
    "# -----------------------------------\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 変数をループしてtarget encoding\n",
    "for c in cat_cols:\n",
    "    # 学習データ全体で各カテゴリにおけるtargetの平均を計算\n",
    "    data_tmp = pd.DataFrame({c: train_x[c], 'target': train_y})\n",
    "    target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "    # テストデータのカテゴリを置換\n",
    "    test_x[c] = test_x[c].map(target_mean)\n",
    "\n",
    "    # 学習データの変換後の値を格納する配列を準備\n",
    "    tmp = np.repeat(np.nan, train_x.shape[0])\n",
    "\n",
    "    # 学習データを分割\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=72)\n",
    "    for idx_1, idx_2 in kf.split(train_x):\n",
    "        # out-of-foldで各カテゴリにおける目的変数の平均を計算\n",
    "        target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "        # 変換後の値を一時配列に格納\n",
    "        tmp[idx_2] = train_x[c].iloc[idx_2].map(target_mean)\n",
    "\n",
    "    # 変換後のデータで元の変数を置換\n",
    "    train_x[c] = tmp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2223, number of negative: 6957\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 520\n",
      "[LightGBM] [Info] Number of data points in the train set: 9180, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242157 -> initscore=-1.140891\n",
      "[LightGBM] [Info] Start training from score -1.140891\n",
      "AUC:0.8421524006054544, precision:0.7872340425531915, recall:0.7489878542510121\n",
      "[LightGBM] [Info] Number of positive: 2223, number of negative: 6957\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 521\n",
      "[LightGBM] [Info] Number of data points in the train set: 9180, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242157 -> initscore=-1.140891\n",
      "[LightGBM] [Info] Start training from score -1.140891\n",
      "AUC:0.829131990090661, precision:0.7330677290836654, recall:0.7449392712550608\n",
      "[LightGBM] [Info] Number of positive: 2223, number of negative: 6957\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 522\n",
      "[LightGBM] [Info] Number of data points in the train set: 9180, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242157 -> initscore=-1.140891\n",
      "[LightGBM] [Info] Start training from score -1.140891\n",
      "AUC:0.8371453561757912, precision:0.7450199203187251, recall:0.757085020242915\n",
      "[LightGBM] [Info] Number of positive: 2223, number of negative: 6957\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 521\n",
      "[LightGBM] [Info] Number of data points in the train set: 9180, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242157 -> initscore=-1.140891\n",
      "[LightGBM] [Info] Start training from score -1.140891\n",
      "AUC:0.7960126956858761, precision:0.7198275862068966, recall:0.6761133603238867\n",
      "[LightGBM] [Info] Number of positive: 2223, number of negative: 6957\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000288 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 521\n",
      "[LightGBM] [Info] Number of data points in the train set: 9180, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242157 -> initscore=-1.140891\n",
      "[LightGBM] [Info] Start training from score -1.140891\n",
      "AUC:0.829131990090661, precision:0.7330677290836654, recall:0.7449392712550608\n",
      "[LightGBM] [Info] Number of positive: 2223, number of negative: 6957\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 522\n",
      "[LightGBM] [Info] Number of data points in the train set: 9180, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242157 -> initscore=-1.140891\n",
      "[LightGBM] [Info] Start training from score -1.140891\n",
      "AUC:0.8327851422765292, precision:0.7105263157894737, recall:0.7651821862348178\n",
      "[LightGBM] [Info] Number of positive: 2223, number of negative: 6957\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 520\n",
      "[LightGBM] [Info] Number of data points in the train set: 9180, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242157 -> initscore=-1.140891\n",
      "[LightGBM] [Info] Start training from score -1.140891\n",
      "AUC:0.8270238986859128, precision:0.7398373983739838, recall:0.7368421052631579\n",
      "[LightGBM] [Info] Number of positive: 2223, number of negative: 6957\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 520\n",
      "[LightGBM] [Info] Number of data points in the train set: 9180, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242157 -> initscore=-1.140891\n",
      "[LightGBM] [Info] Start training from score -1.140891\n",
      "AUC:0.8368939564554734, precision:0.7698744769874477, recall:0.7449392712550608\n",
      "[LightGBM] [Info] Number of positive: 2223, number of negative: 6957\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 522\n",
      "[LightGBM] [Info] Number of data points in the train set: 9180, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242157 -> initscore=-1.140891\n",
      "[LightGBM] [Info] Start training from score -1.140891\n",
      "AUC:0.8318869120258103, precision:0.7294117647058823, recall:0.7530364372469636\n",
      "[LightGBM] [Info] Number of positive: 2223, number of negative: 6957\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 521\n",
      "[LightGBM] [Info] Number of data points in the train set: 9180, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242157 -> initscore=-1.140891\n",
      "[LightGBM] [Info] Start training from score -1.140891\n",
      "AUC:0.8410263393581975, precision:0.763265306122449, recall:0.757085020242915\n",
      "Kfold平均 AUC:0.8303190681450368, precision:0.743113226922538, recall:0.7429149797570851\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def get_evaluate(y_test, predict):\n",
    "\n",
    "    fpr, tpr, thr_arr = metrics.roc_curve(y_test, predict)\n",
    "\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    precision = metrics.precision_score(y_test, predict)\n",
    "    recall = metrics.recall_score(y_test, predict)      \n",
    "\n",
    "    return auc, precision, recall\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# kfoldの分割数\n",
    "k = 10\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=0)\n",
    "\n",
    "lgbm_params = {'objective': 'binary'}\n",
    "\n",
    "auc_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "\n",
    "X = train_x\n",
    "y = train_y\n",
    "\n",
    "# kfoldで分割\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "\n",
    "    # データセットを生成する\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "    # 学習\n",
    "    model = lgb.train(lgbm_params, lgb_train)\n",
    "\n",
    "    predict_proba = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    # predict_probaが0.5以上なら1とする\n",
    "    predict = [0 if i < 0.5 else 1 for i in predict_proba]\n",
    "\n",
    "    auc, precision, recall = get_evaluate(y_test, predict)\n",
    "\n",
    "    print('AUC:{}, precision:{}, recall:{}'.format(auc, precision, recall))\n",
    "\n",
    "    auc_list.append(auc)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "\n",
    "#kfoldの平均値を取得\n",
    "print('Kfold平均 AUC:{}, precision:{}, recall:{}'.format(np.mean(auc_list), \n",
    "                                                         np.mean(precision_list), \n",
    "                                                         np.mean(recall_list)))\n",
    "                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import catboost as cb\n",
    "from catboost import CatBoost\n",
    "from catboost import Pool\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "C:/Program Files (x86)/Go Agent/pipelines/BuildMaster/catboost.git/tools/enum_parser/enum_serialization_runtime/enum_runtime.cpp:32: Key 'AUCE' not found in enum ELossFunction. Valid options are: 'Logloss', 'CrossEntropy', 'CtrFactor', 'RMSE', 'Lq', 'MAE', 'Quantile', 'Expectile', 'LogLinQuantile', 'MAPE', 'Poisson', 'MSLE', 'MedianAbsoluteError', 'SMAPE', 'Huber', 'Tweedie', 'Cox', 'RMSEWithUncertainty', 'MultiClass', 'MultiClassOneVsAll', 'PairLogit', 'PairLogitPairwise', 'YetiRank', 'YetiRankPairwise', 'QueryRMSE', 'QuerySoftMax', 'QueryCrossEntropy', 'StochasticFilter', 'LambdaMart', 'StochasticRank', 'PythonUserDefinedPerObject', 'PythonUserDefinedMultiRegression', 'UserPerObjMetric', 'UserQuerywiseMetric', 'R2', 'NumErrors', 'FairLoss', 'AUC', 'Accuracy', 'BalancedAccuracy', 'BalancedErrorRate', 'BrierScore', 'Precision', 'Recall', 'F1', 'TotalF1', 'MCC', 'ZeroOneLoss', 'HammingLoss', 'HingeLoss', 'Kappa', 'WKappa', 'LogLikelihoodOfPrediction', 'NormalizedGini', 'PRAUC', 'PairAccuracy', 'AverageGain', 'QueryAverage', 'QueryAUC', 'PFound', 'PrecisionAt', 'RecallAt', 'MAP', 'NDCG', 'DCG', 'FilteredDCG', 'MRR', 'ERR', 'SurvivalAft', 'MultiRMSE', 'MultiRMSEWithMissingValues', 'Combination'. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-fc2ef0901589>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# 上記のパラメータでモデルを学習する\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCatBoost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Science for Kaggle\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mCatBoost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \"\"\"\n\u001b[1;32m-> 2146\u001b[1;33m         return self._fit(X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id,\n\u001b[0m\u001b[0;32m   2147\u001b[0m                          \u001b[0mpairs_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_best_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m                          \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Science for Kaggle\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   1979\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y may be None only when X is an instance of catboost.Pool or string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1981\u001b[1;33m         train_params = self._prepare_train_params(\n\u001b[0m\u001b[0;32m   1982\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1983\u001b[0m             \u001b[0mpairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Science for Kaggle\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_prepare_train_params\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[0;32m   1911\u001b[0m         \u001b[0m_check_param_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_params_type_cast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1913\u001b[1;33m         \u001b[0m_check_train_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0meval_set_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_set\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._check_train_params\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._check_train_params\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: C:/Program Files (x86)/Go Agent/pipelines/BuildMaster/catboost.git/tools/enum_parser/enum_serialization_runtime/enum_runtime.cpp:32: Key 'AUCE' not found in enum ELossFunction. Valid options are: 'Logloss', 'CrossEntropy', 'CtrFactor', 'RMSE', 'Lq', 'MAE', 'Quantile', 'Expectile', 'LogLinQuantile', 'MAPE', 'Poisson', 'MSLE', 'MedianAbsoluteError', 'SMAPE', 'Huber', 'Tweedie', 'Cox', 'RMSEWithUncertainty', 'MultiClass', 'MultiClassOneVsAll', 'PairLogit', 'PairLogitPairwise', 'YetiRank', 'YetiRankPairwise', 'QueryRMSE', 'QuerySoftMax', 'QueryCrossEntropy', 'StochasticFilter', 'LambdaMart', 'StochasticRank', 'PythonUserDefinedPerObject', 'PythonUserDefinedMultiRegression', 'UserPerObjMetric', 'UserQuerywiseMetric', 'R2', 'NumErrors', 'FairLoss', 'AUC', 'Accuracy', 'BalancedAccuracy', 'BalancedErrorRate', 'BrierScore', 'Precision', 'Recall', 'F1', 'TotalF1', 'MCC', 'ZeroOneLoss', 'HammingLoss', 'HingeLoss', 'Kappa', 'WKappa', 'LogLikelihoodOfPrediction', 'NormalizedGini', 'PRAUC', 'PairAccuracy', 'AverageGain', 'QueryAverage', 'QueryAUC', 'PFound', 'PrecisionAt', 'RecallAt', 'MAP', 'NDCG', 'DCG', 'FilteredDCG', 'MRR', 'ERR', 'SurvivalAft', 'MultiRMSE', 'MultiRMSEWithMissingValues', 'Combination'. "
     ]
    }
   ],
   "source": [
    "# データセットを生成する\n",
    "train_pool = Pool(train_x, train_y)\n",
    "test_pool = Pool(X_test, y_test)\n",
    "\n",
    "# LightGBMのハイパーパラメータを設定する\n",
    "params = {'loss_function': 'AUCE',\n",
    "         'num_boost_round': 100,}\n",
    "\n",
    "# 上記のパラメータでモデルを学習する\n",
    "model = CatBoost(params)\n",
    "model.fit(train_pool)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# RMSE を計算する\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:',round(rmse,3))\n",
    "\"\"\"output\n",
    "RMSE: 2.947\n",
    "\"\"\"\n",
    "\n",
    "# 特徴量の重要度を取得する\n",
    "feature_importance = model.get_feature_importance()\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "import numpy as np\n",
    "\n",
    "# カテゴリのカラムのみを抽出\n",
    "categorical_features_indices = np.where(train_x.dtypes != np.float)[0]\n",
    "\n",
    "# データセットの作成。Poolで説明変数、目的変数、\n",
    "# カラムのデータ型を指定できる\n",
    "train_pool = Pool(train_x, train_y, cat_features=categorical_features_indices)\n",
    "validate_pool = Pool(test_x, y_test, cat_features=categorical_features_indices)\n",
    "\n",
    "# 分類用のインスタンスを作成\n",
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(custom_loss=['Accuracy'],\n",
    "                           random_seed=42)\n",
    "\n",
    "# CatBoost, CatBoostRegressorも存在するが損失関数が異なるだけ\n",
    "# CatBoostClassifierの損失関数はlog_lossデフォルト\n",
    "\n",
    "model.fit(train_pool, \n",
    "          eval_set=validate_pool,    # 検証用データ\n",
    "          early_stopping_rounds=10,  # 10回以上精度が改善しなければ中止\n",
    "          use_best_model=True,       # 最も精度が高かったモデルを使用するかの設定\n",
    "          plot=True)                 # 誤差の推移を描画するか否かの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ced18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x[num_cols])\n",
    "\n",
    "train_x[num_cols] = scaler.transform(train_x[num_cols])\n",
    "test_x[num_cols] = scaler.transform(test_x[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e59ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# 前処理\n",
    "df = train\n",
    "df[\"AG_ratio\"].fillna(df[\"Alb\"] / (df[\"TP\"] - df[\"Alb\"]), inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "X = df.drop([\"disease\"], axis=1)\n",
    "y = df[\"disease\"]\n",
    "\n",
    "# Gender列を除外（数量変数のデータに絞る）\n",
    "X_target = X.drop([\"Gender\"], axis=1)\n",
    "\n",
    "# 多項式・交互作用特徴量の生成\n",
    "polynomial = PolynomialFeatures(degree=2, include_bias=False)\n",
    "polynomial_arr = polynomial.fit_transform(X_target)\n",
    "\n",
    "# polynomial_arrのデータフレーム化 （※カラムはshape[1]でpolynomial_arrの列数分だけ出力）\n",
    "X_polynomial = pd.DataFrame(polynomial_arr, columns=[\"poly\" + str(x) for x in range(polynomial_arr.shape[1])])\n",
    "\n",
    "\n",
    "# 生成した多項式・交互作用特徴量の表示\n",
    "print(X_polynomial.shape)\n",
    "print(X_polynomial.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#診断ガイドラインを参照して差分の特徴量を作成する\n",
    "\n",
    "#AST_GOT\n",
    "train_x.loc[train_x['AST_GOT'] > 38, 'AST_GOT'] = train_x['AST_GOT'] - 38\n",
    "train_x.loc[~(train_x['AST_GOT'] > 38), 'AST_GOT'] = 0\n",
    "\n",
    "test_x.loc[train_x['AST_GOT'] > 38, 'AST_GOT'] = test_x['AST_GOT'] - 38\n",
    "test_x.loc[~(train_x['AST_GOT'] > 38), 'AST_GOT'] = 0\n",
    "\n",
    "#ALT_GPT\n",
    "train_x.loc[train_x['ALT_GPT'] > 44, 'ALT_GPT'] = train_x['ALT_GPT'] - 44\n",
    "train_x.loc[~(train_x['ALT_GPT'] > 44), 'ALT_GPT'] = 0\n",
    "\n",
    "test_x.loc[train_x['ALT_GPT'] > 38, 'ALT_GPT'] = test_x['ALT_GPT'] - 44\n",
    "test_x.loc[~(train_x['ALT_GPT'] > 38), 'ALT_GPT'] = 0\n",
    "\n",
    "#AG_ratio\n",
    "train_x.loc[train_x['AG_ratio'] > 1.8, 'AG_ratio'] = train_x['AG_ratio'] - 1.8\n",
    "train_x.loc[~(train_x['AG_ratio'] > 1.8), 'AG_ratio'] = 0\n",
    "\n",
    "test_x.loc[train_x['AG_ratio'] > 1.8, 'AG_ratio'] = test_x['AG_ratio'] - 1.8\n",
    "test_x.loc[~(train_x['AG_ratio'] > 1.8), 'AG_ratio'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643252f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# 組み込み法に使うモデルの指定\n",
    "fs_model = LogisticRegression(penalty='l1', random_state=0)\n",
    "\n",
    "# 閾値の指定\n",
    "fs_threshold = \"mean\"\n",
    "\n",
    "# 組み込み法モデルの初期化\n",
    "selector = SelectFromModel(fs_model, threshold=fs_threshold)\n",
    "\n",
    "\n",
    "# ライブラリのimport\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# 組み込み法のモデル、閾値の指定\n",
    "fs_model = LogisticRegression(penalty='l2', random_state=0)\n",
    "fs_threshold = \"mean\"\n",
    "# 組み込み法モデルの初期化\n",
    "selector = SelectFromModel(fs_model, threshold=fs_threshold)\n",
    "\n",
    "# 特徴量選択の実行\n",
    "selector.fit(X_polynomial, y)\n",
    "mask = selector.get_support()\n",
    "\n",
    "# 選択された特徴量だけのサンプル取得\n",
    "X_polynomial_masked = X_polynomial.loc[:, mask]\n",
    "\n",
    "print(\"選択された特徴量の表示（最初の5行）\")\n",
    "print(X_polynomial_masked.head())\n",
    "print(\"選択された特徴量の数の確認\")\n",
    "print(X_polynomial_masked.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# 学習用・評価用データの分割（元の説明変数Xの代わりに、特徴量選択後のX_polynomial_maskedを使う）\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_polynomial_masked, y, test_size=0.3, random_state=0)\n",
    "train_x, test_x, train_x, test_y = train_test_split(X_polynomial_masked, y, test_size=0.3, random_state=0)\n",
    "# モデルの学習・予測\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e53f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#light gbm\n",
    "\"\"\"\n",
    "from optuna.integration import lightgbm as lgb\n",
    "ts = time.time()\n",
    "\n",
    "dtrain = lgb.Dataset(x_train, label=y_train)\n",
    "eval_data = lgb.Dataset(x_val, label=y_val)\n",
    "\n",
    "param = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "    }\n",
    "\n",
    "best = lgb.train(param, \n",
    "                 dtrain,\n",
    "                 valid_sets=eval_data,\n",
    "                 early_stopping_rounds=100)\n",
    "\n",
    "time.time() - ts\n",
    "\n",
    "\"\"\"\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def get_evaluate(y_test, predict):\n",
    "\n",
    "    fpr, tpr, thr_arr = metrics.roc_curve(y_test, predict)\n",
    "\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    precision = metrics.precision_score(y_test, predict)\n",
    "    recall = metrics.recall_score(y_test, predict)      \n",
    "\n",
    "    return auc, precision, recall\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# kfoldの分割数\n",
    "k = 2\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=0)\n",
    "\n",
    "lgbm_params = {'objective': 'binary'}\n",
    "\n",
    "auc_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "\n",
    "X = train_x\n",
    "y = train_y\n",
    "\n",
    "# kfoldで分割\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "\n",
    "    # データセットを生成する\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "    # 学習\n",
    "    model = lgb.train(lgbm_params, lgb_train)\n",
    "\n",
    "    predict_proba = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    # predict_probaが0.5以上なら1とする\n",
    "    predict = [0 if i < 0.5 else 1 for i in predict_proba]\n",
    "\n",
    "    auc, precision, recall = get_evaluate(y_test, predict)\n",
    "\n",
    "    print('AUC:{}, precision:{}, recall:{}'.format(auc, precision, recall))\n",
    "\n",
    "    auc_list.append(auc)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "\n",
    "#kfoldの平均値を取得\n",
    "print('Kfold平均 AUC:{}, precision:{}, recall:{}'.format(np.mean(auc_list), \n",
    "                                                         np.mean(precision_list), \n",
    "                                                         np.mean(recall_list)))\n",
    "                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a465558",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ROC曲線の描画（偽陽性率、真陽性率、閾値の算出）\n",
    "fpr, tpr, thresholds = roc_curve(y_true=test_y, y_score=pred_y)\n",
    "plt.plot(fpr, tpr, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], linestyle=':', label='random')\n",
    "plt.plot([0, 0, 1], [0, 1, 1], linestyle=':', label='ideal')\n",
    "plt.legend()\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.show()\n",
    "\n",
    "# AUCスコアの算出\n",
    "auc_score = roc_auc_score(y_true=y_test, y_score=y_pred)\n",
    "print(\"AUC:\", auc_score)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c0a4b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#catboost\n",
    "\n",
    "from catboost import Pool\n",
    "import numpy as np\n",
    "\n",
    "# カテゴリのカラムのみを抽出\n",
    "categorical_features_indices = np.where(X.dtypes != np.float)[0]\n",
    "\n",
    "# データセットの作成。Poolで説明変数、目的変数、\n",
    "# カラムのデータ型を指定できる\n",
    "train_x = Pool(X_train, y_train, cat_features=categorical_features_indices)\n",
    "validate_pool = Pool(X_test, y_test, cat_features=categorical_features_indices)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_x)\n",
    "pred_label = np.where(pred>0.5,1,0)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission.to_csv('sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fec29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
